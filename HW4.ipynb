{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 43422,
     "status": "ok",
     "timestamp": 1608088633051,
     "user": {
      "displayName": "Ian Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhJOBiWgRRDpM60ltjMoO_lFsG0ZYkKygzCpvnuDA=s64",
      "userId": "00972351256021622399"
     },
     "user_tz": -480
    },
    "id": "F-0Us-OlgZyR"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "#from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer, MinMaxScaler,OneHotEncoder,StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 49671,
     "status": "ok",
     "timestamp": 1608088640570,
     "user": {
      "displayName": "Ian Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhJOBiWgRRDpM60ltjMoO_lFsG0ZYkKygzCpvnuDA=s64",
      "userId": "00972351256021622399"
     },
     "user_tz": -480
    },
    "id": "HRAzjpgugdxe"
   },
   "outputs": [],
   "source": [
    "sc = SparkSession.builder.appName('hw4').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 52970,
     "status": "ok",
     "timestamp": 1608088644849,
     "user": {
      "displayName": "Ian Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhJOBiWgRRDpM60ltjMoO_lFsG0ZYkKygzCpvnuDA=s64",
      "userId": "00972351256021622399"
     },
     "user_tz": -480
    },
    "id": "MeODZMYTgd2L"
   },
   "outputs": [],
   "source": [
    "customSchema = StructType([\n",
    "  StructField(\"Year\", IntegerType(), True),\n",
    "  StructField(\"Month\", IntegerType(), True),\n",
    "  StructField(\"DayofMonth\", IntegerType(), True),\n",
    "  StructField(\"DayOfWeek\", IntegerType(), True),\n",
    "  StructField(\"DepTime\", IntegerType(), True),\n",
    "  StructField(\"CRSDepTime\", IntegerType(), True),\n",
    "  StructField(\"ArrTime\", IntegerType(), True),\n",
    "  StructField(\"CRSArrTime\", IntegerType(), True),\n",
    "  StructField(\"UniqueCarrier\", StringType(), True),\n",
    "  StructField(\"FlightNum\", IntegerType(), True),\n",
    "  StructField(\"TailNum\", StringType(), True),\n",
    "  StructField(\"ActualElapsedTime\", IntegerType(), True),\n",
    "  StructField(\"CRSElapsedTime\", IntegerType(), True),\n",
    "  StructField(\"AirTime\", IntegerType(), True),\n",
    "  StructField(\"ArrDelay\", IntegerType(), True),\n",
    "  StructField(\"DepDelay\", IntegerType(), True),\n",
    "  StructField(\"Origin\", StringType(), True),\n",
    "  StructField(\"Dest\", StringType(), True),\n",
    "  StructField(\"Distance\", IntegerType(), True),\n",
    "  StructField(\"TaxiIn\", IntegerType(), True),\n",
    "  StructField(\"TaxiOut\", IntegerType(), True),\n",
    "  StructField(\"Cancelled\", IntegerType(), True),\n",
    "  StructField(\"CancellationCode\", StringType(), True),\n",
    "  StructField(\"Diverted\", IntegerType(), True),\n",
    "  StructField(\"CarrierDelay\", IntegerType(), True),\n",
    "  StructField(\"WeatherDelay\", IntegerType(), True),\n",
    "  StructField(\"NASDelay\", IntegerType(), True),\n",
    "  StructField(\"SecurityDelay\", IntegerType(), True),\n",
    "  StructField(\"LateAircraftDelay\", IntegerType(), True)]\n",
    ")\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "df0 = sc.read.csv('2000.csv', schema=customSchema, header=True)\n",
    "df1 = sc.read.csv('2001.csv', schema=customSchema, header=True)\n",
    "df2 = sc.read.csv('2002.csv', schema=customSchema, header=True)\n",
    "df3 = sc.read.csv('2003.csv', schema=customSchema, header=True)\n",
    "df4 = sc.read.csv('2004.csv', schema=customSchema, header=True)\n",
    "df5 = sc.read.csv('2005.csv', schema=customSchema, header=True)\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "df = df0.union(df1)\n",
    "df = df.union(df2)\n",
    "df = df.union(df3)\n",
    "df = df.union(df4)\n",
    "df = df.union(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 50913,
     "status": "ok",
     "timestamp": 1608088644849,
     "user": {
      "displayName": "Ian Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhJOBiWgRRDpM60ltjMoO_lFsG0ZYkKygzCpvnuDA=s64",
      "userId": "00972351256021622399"
     },
     "user_tz": -480
    },
    "id": "OvipvqW2iuA6"
   },
   "outputs": [],
   "source": [
    "#df = df0.sample(withReplacement=False, fraction=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqXImOdBjmhN"
   },
   "outputs": [],
   "source": [
    "na_count = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "DT0b3Bflgd5i",
    "outputId": "ce3d4338-0e93-4f83-e519-ba31753f9a76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|   0|    0|         0|        0| 846787|         0| 921499|         0|            0|        0|  56387|           921500|           270| 921620|  921500|  846787|     0|   0|       0|     0|      0|        0|        17771075|       0|    19594928|    19594928|19594928|     19594928|         19594928|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "na_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Vta9l_vIhHnV",
    "outputId": "e49e49a4-ecee-414c-ad7c-e767c58d7097"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15072236.8"
      ]
     },
     "execution_count": 0,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()*0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "v-hB02kQufmr"
   },
   "outputs": [],
   "source": [
    "nalist = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "WYaQX1aO1B9r"
   },
   "outputs": [],
   "source": [
    "someNull_cols = [df.columns[c] for c in range(len(df.columns)) if (nalist[0][c] < df.count()*0.4) & (nalist[0][c] > 0)]\n",
    "nonNull_cols = [df.columns[c] for c in range(len(df.columns)) if (nalist[0][c] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ZesFN90mGeHe",
    "outputId": "a7be6002-99c5-4400-9854-6cbea088dbee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DepTime',\n",
       " 'ArrTime',\n",
       " 'TailNum',\n",
       " 'ActualElapsedTime',\n",
       " 'CRSElapsedTime',\n",
       " 'AirTime',\n",
       " 'ArrDelay',\n",
       " 'DepDelay']"
      ]
     },
     "execution_count": 0,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "someNull_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1608085373610,
     "user": {
      "displayName": "Ian Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhJOBiWgRRDpM60ltjMoO_lFsG0ZYkKygzCpvnuDA=s64",
      "userId": "00972351256021622399"
     },
     "user_tz": -480
    },
    "id": "z4pX7HOIGgOx",
    "outputId": "41150480-d720-4573-b840-e6ee4b9ee646"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year',\n",
       " 'Month',\n",
       " 'DayofMonth',\n",
       " 'DayOfWeek',\n",
       " 'CRSDepTime',\n",
       " 'CRSArrTime',\n",
       " 'UniqueCarrier',\n",
       " 'FlightNum',\n",
       " 'Origin',\n",
       " 'Dest',\n",
       " 'Distance',\n",
       " 'TaxiIn',\n",
       " 'TaxiOut',\n",
       " 'Cancelled',\n",
       " 'Diverted']"
      ]
     },
     "execution_count": 0,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonNull_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "XKmagdjNr4qt"
   },
   "outputs": [],
   "source": [
    "#someNull_cols = [c for c in na_count.columns if (na_count[[c]].first()[c] < 15072237) & (na_count[[c]].first()[c] > 0)]\n",
    "#nonNull_cols = [c for c in na_count.columns if na_count[[c]].first()[c] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "EqWvePVso6R5",
    "outputId": "f78ebf87-baf3-4ca1-c62a-6d5e6a8b4be9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|TailNum|count|\n",
      "+-------+-----+\n",
      "|  N6700| 6922|\n",
      "| N919UA| 8899|\n",
      "| N499AA| 6509|\n",
      "| N502US| 5876|\n",
      "|   N656| 8272|\n",
      "| N516UA| 5381|\n",
      "| N513UA| 5505|\n",
      "| N102UW| 6742|\n",
      "| N912TW| 1655|\n",
      "| N385US| 8785|\n",
      "| N240AU| 1909|\n",
      "| N411US| 2927|\n",
      "| N902DE| 8879|\n",
      "| N407AA| 6237|\n",
      "| N33637| 7436|\n",
      "| N607NW| 7429|\n",
      "| N745AS| 8989|\n",
      "| N567AA| 5676|\n",
      "| N2CAAA| 3038|\n",
      "| N201US| 1344|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('TailNum').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 688,
     "status": "ok",
     "timestamp": 1608088701796,
     "user": {
      "displayName": "Ian Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhJOBiWgRRDpM60ltjMoO_lFsG0ZYkKygzCpvnuDA=s64",
      "userId": "00972351256021622399"
     },
     "user_tz": -480
    },
    "id": "ddK9Whm3sK6H"
   },
   "outputs": [],
   "source": [
    "someNull_cols.remove('TailNum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IsRlmc7Lgd8V"
   },
   "outputs": [],
   "source": [
    "for i in someNull_cols:\n",
    "    na_mean = df.select(mean(df[i])).collect()\n",
    "    df = df.na.fill(na_mean[0][0],subset=[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IsRlmc7Lgd8V"
   },
   "outputs": [],
   "source": [
    "clean_cols = nonNull_cols+someNull_cols\n",
    "df = df.select(*clean_cols)\n",
    "df = df.withColumnRenamed(\"Cancelled\",\"label\")\n",
    "\n",
    "stages = []\n",
    "str_col = ['UniqueCarrier','Origin','Dest',\"FlightNum\"]\n",
    "for i in str_col:\n",
    "    stringIndexer = StringIndexer(inputCol = i,outputCol = i+'Index')\n",
    "    encoder_str = OneHotEncoder(inputCols = [stringIndexer.getOutputCol()],outputCols = [i+\"_str_cat\"])\n",
    "    stages += [stringIndexer, encoder_str]\n",
    "\n",
    "cat_col = [\"Month\",\"DayofMonth\",\"DayOfWeek\",\"Diverted\"]\n",
    "encoder_cat = OneHotEncoder(inputCols = cat_col,outputCols = [i+\"_cat\" for i in cat_col])\n",
    "stages += [encoder_cat]\n",
    "\n",
    "conti_col = [\"DepTime\",\"ArrTime\",\"ActualElapsedTime\",\"CRSElapsedTime\",\"AirTime\",\"ArrDelay\",\"DepDelay\",\\\n",
    "             \"CRSDepTime\",\"CRSArrTime\",\"Distance\",\"TaxiIn\",\"TaxiOut\"]\n",
    "numVect = VectorAssembler(inputCols = conti_col, outputCol=\"numFeatures\")\n",
    "scaler = StandardScaler(inputCol=numVect.getOutputCol(), outputCol=\"scaledFeatures\")\n",
    "stages += [numVect, scaler]\n",
    "\n",
    "all_col = [i+\"_str_cat\" for i in str_col] + [i+\"_cat\" for i in cat_col]+[\"scaledFeatures\"]\n",
    "assembler = VectorAssembler(inputCols=all_col, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(df)\n",
    "df_clean = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IsRlmc7Lgd8V",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1235.fit.\n: org.apache.spark.SparkException: Job 16 cancelled because SparkContext was shut down\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:979)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:977)\r\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\r\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:977)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2257)\r\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\r\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2170)\r\n\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:1973)\r\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1357)\r\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1973)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:631)\r\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\r\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\r\n\tat java.util.concurrent.FutureTask.run(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2194)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1157)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1151)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1220)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1196)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:499)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:487)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:482)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:281)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:150)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-428ac77ac59f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'features'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabelCol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'label'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxIter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mregParam\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mlrModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;31m#lr = LogisticRegression(featuresCol = 'features', labelCol = 'label')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#lr_pipeline = Pipeline(stages = [lr])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark\\spark\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    127\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mD:\\spark\\spark\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark\\spark\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    316\u001b[0m         \"\"\"\n\u001b[0;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark\\spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark\\spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1235.fit.\n: org.apache.spark.SparkException: Job 16 cancelled because SparkContext was shut down\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:979)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:977)\r\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\r\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:977)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2257)\r\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\r\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2170)\r\n\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:1973)\r\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1357)\r\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1973)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:631)\r\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\r\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\r\n\tat java.util.concurrent.FutureTask.run(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2194)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1157)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1151)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1220)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1196)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:499)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:487)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:482)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:281)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:150)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "train = df_clean.filter(df_clean.Year < 2005)\n",
    "test = df_clean.filter(df_clean.Year == 2005)\n",
    "test = test.withColumnRenamed(\"label\",\"trueLabel\")\n",
    "\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label')\n",
    "\n",
    "grid = ParamGridBuilder().addGrid(lr.regParam, [0.01,1]).build().addGrid(lr.maxIter, [10, 20]).build()\n",
    "cv = CrossValidator(estimator = lr_pipeline,estimatorParamMaps = grid,evaluator = BinaryClassificationEvaluator(),numFolds = 3)\n",
    "cvModel = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QBB50jm8h1Y"
   },
   "outputs": [],
   "source": [
    "prediction = cvModel.transform(test)\n",
    "predicted = prediction.select(\"prediction\", \"trueLabel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "executionInfo": {
     "elapsed": 1816337,
     "status": "error",
     "timestamp": 1608055163995,
     "user": {
      "displayName": "Ian Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhJOBiWgRRDpM60ltjMoO_lFsG0ZYkKygzCpvnuDA=s64",
      "userId": "00972351256021622399"
     },
     "user_tz": -480
    },
    "id": "Pk_456Fx8t62",
    "outputId": "0723e0eb-0c4f-4966-ce9d-26c0cad8f21b"
   },
   "outputs": [],
   "source": [
    "tp = float(prediction.filter(\"prediction == 1.0 AND truelabel == 1\").count())\n",
    "fp = float(prediction.filter(\"prediction == 1.0 AND truelabel == 0\").count())\n",
    "tn = float(prediction.filter(\"prediction == 0.0 AND truelabel == 0\").count())\n",
    "fn = float(prediction.filter(\"prediction == 0.0 AND truelabel == 1\").count())\n",
    "pr = tp / (tp + fp)\n",
    "re = tp / (tp + fn)\n",
    "metrics = spark.createDataFrame([\n",
    " (\"TP\", tp),\n",
    " (\"FP\", fp),\n",
    " (\"TN\", tn),\n",
    " (\"FN\", fn),\n",
    " (\"Precision\", pr),\n",
    " (\"Recall\", re),\n",
    " (\"F1\", 2*pr*re/(re+pr)),\n",
    " ],[\"metric\", \"value\"])\n",
    "metrics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEvEA5op80sE"
   },
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol=\"trueLabel\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(prediction)\n",
    "print( \"AUC = \", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMFWg7pr_uud"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 110573,
     "status": "ok",
     "timestamp": 1607620743361,
     "user": {
      "displayName": "Ian Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhJOBiWgRRDpM60ltjMoO_lFsG0ZYkKygzCpvnuDA=s64",
      "userId": "00972351256021622399"
     },
     "user_tz": -480
    },
    "id": "NjT8fcSWspRo",
    "outputId": "2b624c83-4333-4d53-a2d0-fe03f8cedcf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|   metric|               value|\n",
      "+---------+--------------------+\n",
      "|       TP|                 2.0|\n",
      "|       FP|                 8.0|\n",
      "|       TN|              1621.0|\n",
      "|       FN|                53.0|\n",
      "| Accuracy|  0.9637767220902613|\n",
      "|Precision|                 0.2|\n",
      "|   Recall| 0.03636363636363636|\n",
      "|       F1|0.061538461538461535|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvPrediction = cvModel.transform(test)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "tp2 = float(cvPrediction.filter(\"prediction == 1.0 AND truelabel == 1\").count())\n",
    "fp2 = float(cvPrediction.filter(\"prediction == 1.0 AND truelabel == 0\").count())\n",
    "tn2 = float(cvPrediction.filter(\"prediction == 0.0 AND truelabel == 0\").count())\n",
    "fn2 = float(cvPrediction.filter(\"prediction == 0.0 AND truelabel == 1\").count())\n",
    "pr2 = tp2 / (tp2 + fp2)\n",
    "re2 = tp2 / (tp2 + fn2)\n",
    "acc2 = (tp2+tn2)/(tp2+fp2+tn2+fn2)\n",
    "metrics2 = sc.createDataFrame([\n",
    " (\"TP\", tp2),\n",
    " (\"FP\", fp2),\n",
    " (\"TN\", tn2),\n",
    " (\"FN\", fn2),\n",
    " (\"Accuracy\", acc2),\n",
    " (\"Precision\", pr2),\n",
    " (\"Recall\", re2),\n",
    " (\"F1\", 2*pr2*re2/(re2+pr2))],[\"metric\", \"value\"])\n",
    "metrics2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34795,
     "status": "ok",
     "timestamp": 1607621100734,
     "user": {
      "displayName": "Ian Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhJOBiWgRRDpM60ltjMoO_lFsG0ZYkKygzCpvnuDA=s64",
      "userId": "00972351256021622399"
     },
     "user_tz": -480
    },
    "id": "WSOvpRTJspih",
    "outputId": "375147cb-7554-4685-cea0-e60107a14f30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC =  0.5157263240136168\n"
     ]
    }
   ],
   "source": [
    "evaluator2 = BinaryClassificationEvaluator(labelCol=\"trueLabel\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "auc2 = evaluator2.evaluate(cvPrediction)\n",
    "print( \"AUC = \", auc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZynA8Tvpctd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP2fk01KmaWEraWIGUJPU3V",
   "mount_file_id": "1Bz2Jvfg6RTTvo_3Xg3Ocl964VwObjwJ_",
   "name": "HW4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
